Use EMA with decay: 0.9996
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
loader train len: 10
loader eval len: 4
Eval checkpoint: /home/gcperkins/model_best.pth.tar
Confirmed checkpoint path: /home/gcperkins/model_best.pth.tar
Miss Keys: []
Ubexpected Keys: ['head.weight', 'head.bias']
Sequential(
  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
  (1): Rearrange('b c h w -> b (h w) c', h=56, w=56)
  (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=64, out_features=192, bias=True)
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=64, out_features=64, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): Identity()
    (mlp): Mlp(
      (fc1): Linear(in_features=64, out_features=256, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=256, out_features=64, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
)
Merge_Block(
  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=128, out_features=384, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=128, out_features=128, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=128, out_features=512, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=512, out_features=128, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (1): CSWinBlock(
    (qkv): Linear(in_features=128, out_features=384, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=128, out_features=128, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=128, out_features=512, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=512, out_features=128, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
)
Merge_Block(
  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (1): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (2): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (3): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (4): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (5): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (6): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (7): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (8): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (9): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (10): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (11): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (12): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (13): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (14): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (15): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (16): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (17): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (18): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (19): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (20): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)
Merge_Block(
  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=512, out_features=1536, bias=True)
    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=512, out_features=512, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=512, out_features=2048, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=2048, out_features=512, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Linear(in_features=512, out_features=4, bias=True)
LayerNorm((512,), eps=1e-05, elementwise_affine=True)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(0)
/home/gcperkins/full_size_pu/val/1/stage_1_56450.jpg
input shape torch.Size([1, 3, 224, 224])
result shape (224, 224, 1)
