Use EMA with decay: 0.9996
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
loader train len: 10
loader eval len: 4
Eval checkpoint: /home/gcperkins/model_best.pth.tar
Confirmed checkpoint path: /home/gcperkins/model_best.pth.tar
Miss Keys: []
Ubexpected Keys: ['head.weight', 'head.bias']
Sequential(
  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
  (1): Rearrange('b c h w -> b (h w) c', h=56, w=56)
  (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=64, out_features=192, bias=True)
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=64, out_features=64, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): Identity()
    (mlp): Mlp(
      (fc1): Linear(in_features=64, out_features=256, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=256, out_features=64, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
)
Merge_Block(
  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=128, out_features=384, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=128, out_features=128, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=128, out_features=512, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=512, out_features=128, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (1): CSWinBlock(
    (qkv): Linear(in_features=128, out_features=384, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=128, out_features=128, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=128, out_features=512, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=512, out_features=128, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
)
Merge_Block(
  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (1): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (2): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (3): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (4): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (5): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (6): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (7): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (8): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (9): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (10): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (11): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (12): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (13): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (14): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (15): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (16): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (17): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (18): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (19): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (20): CSWinBlock(
    (qkv): Linear(in_features=256, out_features=768, bias=True)
    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=256, out_features=256, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
      (1): LePEAttention(
        (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=256, out_features=1024, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=1024, out_features=256, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
)
Merge_Block(
  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
ModuleList(
  (0): CSWinBlock(
    (qkv): Linear(in_features=512, out_features=1536, bias=True)
    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (proj): Linear(in_features=512, out_features=512, bias=True)
    (proj_drop): Dropout(p=0.0, inplace=False)
    (attns): ModuleList(
      (0): LePEAttention(
        (get_v): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
        (attn_drop): Dropout(p=0.0, inplace=False)
      )
    )
    (drop_path): DropPath()
    (mlp): Mlp(
      (fc1): Linear(in_features=512, out_features=2048, bias=True)
      (act): GELU(approximate='none')
      (fc2): Linear(in_features=2048, out_features=512, bias=True)
      (drop): Dropout(p=0.0, inplace=False)
    )
    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
LayerNorm((512,), eps=1e-05, elementwise_affine=True)
Linear(in_features=512, out_features=4, bias=True)
LayerNorm((256,), eps=1e-05, elementwise_affine=True)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(0)
/home/gcperkins/full_size_pu/val/1/stage_1_56450.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(1)
/home/gcperkins/full_size_pu/val/1/stage_1_56981.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(2)
/home/gcperkins/full_size_pu/val/1/stage_1_57315.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(3)
/home/gcperkins/full_size_pu/val/1/stage_1_5767.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(4)
/home/gcperkins/full_size_pu/val/1/stage_1_57710.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(5)
/home/gcperkins/full_size_pu/val/1/stage_1_57860.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(6)
/home/gcperkins/full_size_pu/val/1/stage_1_58048.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(7)
/home/gcperkins/full_size_pu/val/1/stage_1_58672.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(8)
/home/gcperkins/full_size_pu/val/1/stage_1_58916.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(9)
/home/gcperkins/full_size_pu/val/1/stage_1_59646.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(10)
/home/gcperkins/full_size_pu/val/1/stage_1_59688.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(11)
/home/gcperkins/full_size_pu/val/1/stage_1_59706.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(12)
/home/gcperkins/full_size_pu/val/1/stage_1_59723.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(13)
/home/gcperkins/full_size_pu/val/1/stage_1_61844.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(14)
/home/gcperkins/full_size_pu/val/1/stage_1_62380.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(15)
/home/gcperkins/full_size_pu/val/1/stage_1_62707.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(16)
/home/gcperkins/full_size_pu/val/1/stage_1_64066.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(17)
/home/gcperkins/full_size_pu/val/1/stage_1_64340.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(18)
/home/gcperkins/full_size_pu/val/1/stage_1_64761.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(19)
/home/gcperkins/full_size_pu/val/1/stage_1_64902.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(20)
/home/gcperkins/full_size_pu/val/1/stage_1_64923.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(21)
/home/gcperkins/full_size_pu/val/1/stage_1_65864.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(22)
/home/gcperkins/full_size_pu/val/1/stage_1_65926.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(23)
/home/gcperkins/full_size_pu/val/1/stage_1_66343.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(24)
/home/gcperkins/full_size_pu/val/1/stage_1_66363.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(25)
/home/gcperkins/full_size_pu/val/1/stage_1_66376.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(26)
/home/gcperkins/full_size_pu/val/1/stage_1_66861.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(27)
/home/gcperkins/full_size_pu/val/1/stage_1_68177.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(28)
/home/gcperkins/full_size_pu/val/1/stage_1_68506.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(29)
/home/gcperkins/full_size_pu/val/1/stage_1_68712.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(30)
/home/gcperkins/full_size_pu/val/1/stage_1_70402.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(31)
/home/gcperkins/full_size_pu/val/1/stage_1_70472.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(32)
/home/gcperkins/full_size_pu/val/1/stage_1_70859.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(33)
/home/gcperkins/full_size_pu/val/1/stage_1_7130.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(34)
/home/gcperkins/full_size_pu/val/1/stage_1_72077.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(35)
/home/gcperkins/full_size_pu/val/1/stage_1_72437.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(36)
/home/gcperkins/full_size_pu/val/1/stage_1_7278.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(37)
/home/gcperkins/full_size_pu/val/1/stage_1_73024.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(38)
/home/gcperkins/full_size_pu/val/1/stage_1_73442.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(39)
/home/gcperkins/full_size_pu/val/1/stage_1_73778.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(40)
/home/gcperkins/full_size_pu/val/1/stage_1_74150.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(41)
/home/gcperkins/full_size_pu/val/1/stage_1_74220.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(42)
/home/gcperkins/full_size_pu/val/1/stage_1_74781.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(43)
/home/gcperkins/full_size_pu/val/1/stage_1_75743.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(44)
/home/gcperkins/full_size_pu/val/1/stage_1_75775.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(45)
/home/gcperkins/full_size_pu/val/1/stage_1_77827.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(46)
/home/gcperkins/full_size_pu/val/2/stage_2_58817.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(47)
/home/gcperkins/full_size_pu/val/2/stage_2_59338.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(48)
/home/gcperkins/full_size_pu/val/2/stage_2_59382.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(49)
/home/gcperkins/full_size_pu/val/2/stage_2_59422.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(50)
/home/gcperkins/full_size_pu/val/2/stage_2_60264.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(51)
/home/gcperkins/full_size_pu/val/2/stage_2_60269.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(52)
/home/gcperkins/full_size_pu/val/2/stage_2_60339.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(53)
/home/gcperkins/full_size_pu/val/2/stage_2_6073.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(54)
/home/gcperkins/full_size_pu/val/2/stage_2_61228.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(55)
/home/gcperkins/full_size_pu/val/2/stage_2_61858.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(56)
/home/gcperkins/full_size_pu/val/2/stage_2_62041.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(57)
/home/gcperkins/full_size_pu/val/2/stage_2_6212.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(58)
/home/gcperkins/full_size_pu/val/2/stage_2_62171.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(59)
/home/gcperkins/full_size_pu/val/2/stage_2_62396.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(60)
/home/gcperkins/full_size_pu/val/2/stage_2_62455.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(61)
/home/gcperkins/full_size_pu/val/2/stage_2_62524.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(62)
/home/gcperkins/full_size_pu/val/2/stage_2_62700.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
input shape (3, 224, 224)
[1, 3, 224, 224]
tensor(63)
/home/gcperkins/full_size_pu/val/2/stage_2_62883.jpg
torch.Size([1, 3, 224, 224])
(224, 224, 1)
